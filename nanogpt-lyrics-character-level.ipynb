{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import libarary\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport pandas as pd\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-10-05T23:55:58.911026Z","iopub.execute_input":"2023-10-05T23:55:58.911356Z","iopub.status.idle":"2023-10-05T23:56:01.986514Z","shell.execute_reply.started":"2023-10-05T23:55:58.911329Z","shell.execute_reply":"2023-10-05T23:56:01.985862Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# read the data\ndata = pd.read_csv('/kaggle/input/150k-lyrics-labeled-with-spotify-valence/labeled_lyrics_cleaned.csv', encoding = 'utf-8')['seq']\ntext = ''.join(data.tolist())","metadata":{"execution":{"iopub.status.busy":"2023-10-05T23:56:01.988419Z","iopub.execute_input":"2023-10-05T23:56:01.989213Z","iopub.status.idle":"2023-10-05T23:56:05.163900Z","shell.execute_reply.started":"2023-10-05T23:56:01.989181Z","shell.execute_reply":"2023-10-05T23:56:05.162923Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(f'number of characters :  {len(text)}')","metadata":{"execution":{"iopub.status.busy":"2023-10-05T23:56:05.165357Z","iopub.execute_input":"2023-10-05T23:56:05.165690Z","iopub.status.idle":"2023-10-05T23:56:05.171221Z","shell.execute_reply.started":"2023-10-05T23:56:05.165660Z","shell.execute_reply":"2023-10-05T23:56:05.169935Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"number of characters :  179890373\n","output_type":"stream"}]},{"cell_type":"code","source":"# make vocab\nchars = sorted(list(set(text)))\n\n#vocab karakter ke index\nstoi = {c:i for i,c in enumerate(chars)}\n\n#vocab index ke karakter\nitos = {i:c for i,c in stoi.items()}\n\nprint(f'jumlah vocab :  {len(stoi)}')\nprint(''.join(chars))","metadata":{"execution":{"iopub.status.busy":"2023-10-05T23:56:05.173685Z","iopub.execute_input":"2023-10-05T23:56:05.174251Z","iopub.status.idle":"2023-10-05T23:56:06.972499Z","shell.execute_reply.started":"2023-10-05T23:56:05.174218Z","shell.execute_reply":"2023-10-05T23:56:06.971340Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"jumlah vocab :  99\n\t\n !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n","output_type":"stream"}]},{"cell_type":"code","source":"stoi = {ch:i for i,ch in enumerate(chars)}\nitos = {i:ch for i,ch in enumerate(chars)}\n\n# fungsi encode dan decode yang mengembalikan list index atau list karakter dari input\nencode = lambda x : [stoi[c] for c in x]\ndecode = lambda x : ''.join([itos[i] for i in x])\n\n\nprint(encode('Hi Yubaba'))\nprint(decode(encode('Hi Yubaba')))","metadata":{"execution":{"iopub.status.busy":"2023-10-05T23:56:06.974019Z","iopub.execute_input":"2023-10-05T23:56:06.974664Z","iopub.status.idle":"2023-10-05T23:56:06.986059Z","shell.execute_reply.started":"2023-10-05T23:56:06.974631Z","shell.execute_reply":"2023-10-05T23:56:06.985085Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[43, 76, 3, 60, 88, 69, 68, 69, 68]\nHi Yubaba\n","output_type":"stream"}]},{"cell_type":"code","source":"# encode entire text then store it into a tensor\ndata = torch.tensor(encode(text), dtype = torch.long)\nprint(data.shape, data.dtype)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T23:56:06.987444Z","iopub.execute_input":"2023-10-05T23:56:06.988007Z","iopub.status.idle":"2023-10-05T23:56:33.531578Z","shell.execute_reply.started":"2023-10-05T23:56:06.987971Z","shell.execute_reply":"2023-10-05T23:56:33.530574Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"torch.Size([179890373]) torch.int64\n","output_type":"stream"}]},{"cell_type":"code","source":"# split train and val dataset\nn = int(0.9 * len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nprint(train_data.shape)\nprint(val_data.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T23:56:33.532999Z","iopub.execute_input":"2023-10-05T23:56:33.533529Z","iopub.status.idle":"2023-10-05T23:56:33.550570Z","shell.execute_reply.started":"2023-10-05T23:56:33.533498Z","shell.execute_reply":"2023-10-05T23:56:33.549351Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"torch.Size([161901335])\ntorch.Size([17989038])\n","output_type":"stream"}]},{"cell_type":"code","source":"block_size = 8\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\n\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f'when input is {context} the target : {target}')","metadata":{"execution":{"iopub.status.busy":"2023-10-05T23:56:33.551899Z","iopub.execute_input":"2023-10-05T23:56:33.552782Z","iopub.status.idle":"2023-10-05T23:56:33.581307Z","shell.execute_reply.started":"2023-10-05T23:56:33.552753Z","shell.execute_reply":"2023-10-05T23:56:33.580420Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"when input is tensor([49]) the target : 82\nwhen input is tensor([49, 82]) the target : 15\nwhen input is tensor([49, 82, 15]) the target : 3\nwhen input is tensor([49, 82, 15,  3]) the target : 81\nwhen input is tensor([49, 82, 15,  3, 81]) the target : 82\nwhen input is tensor([49, 82, 15,  3, 81, 82]) the target : 2\nwhen input is tensor([49, 82, 15,  3, 81, 82,  2]) the target : 1\nwhen input is tensor([49, 82, 15,  3, 81, 82,  2,  1]) the target : 44\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.manual_seed(1337)\n\ndef get_batches(split):\n    # generate small batch of data input x and y\n    if split == 'train':\n        data = train_data\n    else:\n        data = val_data\n\n    ix = torch.randint(low = 0, high = len(data) - block_size, size = (batch_size,)) #generate a tensor contains random integer, used as first index for 1 context sample\n    x = torch.stack([data[i:i+block_size] for i in ix],)\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x , y = x.to(device), y.to(device)\n    return x, y","metadata":{"execution":{"iopub.status.busy":"2023-10-05T23:56:33.582473Z","iopub.execute_input":"2023-10-05T23:56:33.583209Z","iopub.status.idle":"2023-10-05T23:56:33.592933Z","shell.execute_reply.started":"2023-10-05T23:56:33.583180Z","shell.execute_reply":"2023-10-05T23:56:33.592033Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"batch_size = 16\n\nxb, yb = get_batches('train')\nprint(xb)\nprint(yb)\nprint(xb.shape)\nprint(yb.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T23:56:33.596424Z","iopub.execute_input":"2023-10-05T23:56:33.596719Z","iopub.status.idle":"2023-10-05T23:56:36.395044Z","shell.execute_reply.started":"2023-10-05T23:56:33.596698Z","shell.execute_reply":"2023-10-05T23:56:36.394160Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"tensor([[82, 88, 74, 75,  3, 87, 75, 72],\n        [44, 10, 71,  3, 78, 76, 86, 86],\n        [72, 85, 73, 72, 70, 87,  3, 86],\n        [87,  4, 12,  2,  1, 47, 72, 87],\n        [88, 71, 15,  3, 69, 72, 70, 68],\n        [10, 87,  3, 90, 68, 76, 87,  2],\n        [82, 90, 81,  3, 34,  5,  2,  1],\n        [ 3, 71, 82, 90, 81,  3, 76, 81],\n        [42, 85, 72, 74, 82, 85, 92, 15],\n        [71,  3, 69, 76, 74,  3, 82, 79],\n        [86, 87,  3, 75, 68, 86,  3, 85],\n        [ 3, 81, 76, 74, 75, 87, 15,  3],\n        [82, 88,  3, 80, 68, 78, 72,  3],\n        [10, 89, 72,  3, 78, 76, 86, 86],\n        [87, 75, 72,  3, 73, 68, 80, 72],\n        [88, 85,  3, 92, 72, 68, 85, 86]], device='cuda:0')\ntensor([[88, 74, 75,  3, 87, 75, 72,  3],\n        [10, 71,  3, 78, 76, 86, 86,  3],\n        [85, 73, 72, 70, 87,  3, 86, 87],\n        [ 4, 12,  2,  1, 47, 72, 87, 10],\n        [71, 15,  3, 69, 72, 70, 68, 88],\n        [87,  3, 90, 68, 76, 87,  2,  1],\n        [90, 81,  3, 34,  5,  2,  1,  2],\n        [71, 82, 90, 81,  3, 76, 81,  3],\n        [85, 72, 74, 82, 85, 92, 15,  3],\n        [ 3, 69, 76, 74,  3, 82, 79, 71],\n        [87,  3, 75, 68, 86,  3, 85, 88],\n        [81, 76, 74, 75, 87, 15,  3, 87],\n        [88,  3, 80, 68, 78, 72,  3, 80],\n        [89, 72,  3, 78, 76, 86, 86, 72],\n        [75, 72,  3, 73, 68, 80, 72, 15],\n        [85,  3, 92, 72, 68, 85, 86,  3]], device='cuda:0')\ntorch.Size([16, 8])\ntorch.Size([16, 8])\n","output_type":"stream"}]},{"cell_type":"code","source":"block_size = 3 # berapa karakter pendahulu yang dipakai untuk memprediksi karakter setelahnya\n\n#create training set (x,y)\ndef build_dataset(words):\n    X, Y = [], []\n    for w in words:\n        #print(w)\n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            #print(''.join(itos[i] for i in context) , ' ---> ',itos[ix])\n            context = context[1:] + [ix]\n    X = torch.tensor(X, device = device)\n    Y = torch.tensor(Y, device = device)\n    return X,Y","metadata":{"execution":{"iopub.status.busy":"2023-10-05T23:56:36.396316Z","iopub.execute_input":"2023-10-05T23:56:36.397130Z","iopub.status.idle":"2023-10-05T23:56:36.403368Z","shell.execute_reply.started":"2023-10-05T23:56:36.397097Z","shell.execute_reply":"2023-10-05T23:56:36.402398Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\nvocab_size = len(chars)\n\n\n#hyperparamaters\nbatch_size = 64 # how many independent sequence will be process in parallel\nblock_size = 256 # maximum context length\nmax_iters = 20000\neval_interval = 500\nlearning_rate = 3e-4\neval_iters = 200\nn_emb = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2","metadata":{"execution":{"iopub.status.busy":"2023-10-05T23:56:36.404698Z","iopub.execute_input":"2023-10-05T23:56:36.405211Z","iopub.status.idle":"2023-10-05T23:56:36.415684Z","shell.execute_reply.started":"2023-10-05T23:56:36.405175Z","shell.execute_reply":"2023-10-05T23:56:36.414738Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X,Y = get_batches(split)\n            logits, loss = model(X,Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"execution":{"iopub.status.busy":"2023-10-05T23:56:36.417078Z","iopub.execute_input":"2023-10-05T23:56:36.417596Z","iopub.status.idle":"2023-10-05T23:56:36.426417Z","shell.execute_reply.started":"2023-10-05T23:56:36.417567Z","shell.execute_reply":"2023-10-05T23:56:36.425405Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n\ntorch.manual_seed(1337)\n\n\n\nclass MultiHeadAttention(nn.Module):\n    \n    '''ini merupakan implementasi multihead attention tanpa harus membuat class head dan multihead secara terpisah\n    tetapi dimensi head nya dimasukkan seperti batch pada tensornya'''\n    \n    def __init__(self,n_head, head_size):\n        super().__init__()\n        # membuat layer key, query, value dalam 1 linear layer\n        self.kqv = nn.Linear(n_emb, n_emb*3, bias = False) # n_emb*3 karena kita ingin membuat linear layer untuk 3 tensor k, q, v sekaligus\n        self.register_buffer('tril', torch.tril(torch.ones([block_size, block_size]).view(1,1,block_size,block_size))) # pada pytorch kita assign tril seperti ini agar dianggap bukan parameter\n        self.attn_dropout = nn.Dropout(dropout)  # attention layer dropout\n        self.res_dropout = nn.Dropout(dropout)  # residual layer dropout\n        self.proj = nn.Linear(n_emb, n_emb) #projection layer (lihat papernya)\n        \n    def forward(self,x):\n        B,T,C = x.shape\n        k, q, v = self.kqv(x).split(n_emb, dim=2)\n        \n        k = k.view(B,T,n_head, C//n_head).transpose(1,2) #(B,n_head,T,head_size)\n        q = q.view(B,T,n_head, C//n_head).transpose(1,2) #(B,n_head,T,head_size)\n        v = v.view(B,T,n_head, C//n_head).transpose(1,2) #(B,n_head,T,head_size)\n        \n        attn = q @ k.transpose(-2,-1) * (k.shape[-1]**-0.5) # (B,n_head,T, head_size) @ (B,n_head, head_size,T) --> (B,n_head,T,T)\n        attn = attn.masked_fill(self.tril[:,:,:T, :T] == 0, float('-inf')) # (B,n_head,T,T)\n        attn = F.softmax(attn, dim=-1) # (B,n_head,T,T)\n        attn= self.attn_dropout(attn) #(B,n_head,T,T)\n\n        #perform weighted aggregation of v\n        out = attn @ v  #(B,n_head,T,T) @ (B,n_head,T,head_size) --> (B,n_head,T,head_size)\n        out = out.transpose(1,2).contiguous().view(B,T,n_emb)  # menumpuk/concat head size dan n_head di dimensi terakhir menjadi (B,T,head_size*n_head)\n        out = self.res_dropout(self.proj(out))\n        return out\n        \n        \n\n\nclass FeedForward(nn.Module):\n    '''simple linear layer followed by a non linearity\n    lihat paper pada bagian : 3.3 Position-wise Feed-Forward Networks'''\n\n    def __init__(self,n_emb):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_emb, n_emb*4), # pada paper inner layernya lebih besar 4 kali (dari 512 -> 2048) kita coba menirunya\n            nn.ReLU(),\n            nn.Linear(n_emb*4, n_emb), #projection layer setelah feedforward,\n            nn.Dropout(dropout)\n        )\n\n    def forward(self,x):\n        return self.net(x)\n\n\n\nclass Block(nn.Module):\n    '''Transformer block : communication (attention layer) followed by computation(feedforward layer)'''\n    def __init__(self, n_emb, n_head):\n        super().__init__()\n        head_size = n_emb//n_head  #kita mengurangi dimensi dari headnya sesuai dengan jumlah headnya, sehingga computation costnya sama saja dengan single head attention dengan full dimensi\n        self.ma = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_emb)\n        self.layernorm_1 = nn.LayerNorm(n_emb)\n        self.layernorm_2 = nn.LayerNorm(n_emb)\n\n    def forward(self,x):\n        x = x + self.ma(self.layernorm_1(x))  #skip connection and layer norm before attention layer\n        x = x + self.ffwd(self.layernorm_2(x)) #skip connection and layer norm before feedforward layer (lihat paper attention is all you need)\n        return x\n\n\n\nclass LanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n        self.blocks = nn.Sequential(*[Block(n_emb,n_head=n_head) for _ in range(n_layer)])\n        self.lm_head = nn.Linear(n_emb, vocab_size)\n\n    def forward(self, idx, targets = None):\n\n        B,T = idx.shape\n\n        token_emb = self.token_embedding_table(idx)  #(B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device = device))  #(T,C)\n        x = token_emb + pos_emb # (B,T,C)\n        x = self.blocks(x)\n        logits = self.lm_head(x)  #(B,T,vocab_size)\n\n        if targets == None:\n            loss = None\n        else:\n            B, T, C = logits.shape  # (32,8,65)\n            logits =  logits.view(B*T, C) #(32,65)\n            targets = targets.view(B*T) # (32)\n            loss = F.cross_entropy(logits,targets)\n\n        return logits, loss\n\n\n    def generate(self, idx, max_new_tokens):\n        # idx : (B,T) tensor berisi index dari context dari current time step\n        # max_new_tokens : jumlah max karakter yang ingin di generate\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:] #karena menggunakan positional embedding, idx harus dibatasi dengan shifting ketika panjang context sudah lebih dari block_size\n            logits, loss = self(idx_cond)  # prediction\n            logits = logits[:, -1, :] # ambil hanya time step terakhir menjadi (B,C)\n            probs = F.softmax(logits, dim = -1) # softmax untuk mendapatkan distribusi probabilitas untuk setiap class\n            idx_next = torch.multinomial(probs, num_samples=1) # abmil sampel berdasarkan distribusi probabilitas\n            idx = torch.cat((idx, idx_next), dim = 1) # tempelkan ke karakter terakhir\n        return idx\n\nmodel = LanguageModel()\nmodel = model.to(device)\n\n\nlogits,loss = model(xb,yb)\nprint(loss)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T23:56:36.427815Z","iopub.execute_input":"2023-10-05T23:56:36.428113Z","iopub.status.idle":"2023-10-05T23:56:38.310023Z","shell.execute_reply.started":"2023-10-05T23:56:36.428085Z","shell.execute_reply":"2023-10-05T23:56:38.308925Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"tensor(4.9741, device='cuda:0', grad_fn=<NllLossBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T23:56:38.311374Z","iopub.execute_input":"2023-10-05T23:56:38.311727Z","iopub.status.idle":"2023-10-05T23:56:38.316745Z","shell.execute_reply.started":"2023-10-05T23:56:38.311694Z","shell.execute_reply":"2023-10-05T23:56:38.315520Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"for iter in range(max_iters+1):\n\n    #evaluate the loss on train and val\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    #sample batch from data\n    xb, yb = get_batches('train')\n\n    #evaluate the loss\n    logits, loss = model(xb,yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2023-10-05T23:56:38.318174Z","iopub.execute_input":"2023-10-05T23:56:38.318716Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"step 0: train loss 4.8751, val loss 4.8734\nstep 500: train loss 1.9134, val loss 1.9172\nstep 1000: train loss 1.5578, val loss 1.5654\nstep 1500: train loss 1.3998, val loss 1.4081\n","output_type":"stream"}]},{"cell_type":"code","source":"idx = torch.zeros((1,1), dtype = torch.long, device = device) #kick off the generation\nprint(decode(model.generate(idx, max_new_tokens = 2000)[0].tolist()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save model\nstate = {\n    vocab_size : 99,\n    batch_size : 64, # how many independent sequence will be process in parallel\n    block_size : 256, # maximum context length\n    max_iters : 5000,\n    eval_interval : 500,\n    learning_rate : 3e-4,\n    eval_iters : 200,\n    n_emb : 384,\n    n_head : 6,\n    n_layer : 6,\n    dropout : 0.2,\n    optimizer : optimizer.state_dict(),\n    model : model.state_dict()\n}\n\n\ntorch.save(state, '/kaggle/working/GPT_lyrics_2_times_running.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}